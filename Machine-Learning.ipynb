{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boston Housing Dataset\n",
    "\n",
    "The purpose of this assignment is to conduct an analysis on the boston housing dataset. The analysis will review whether there is a significant difference in median house prices along the Charles river and those that aren’t. This assigment will also predict the median house prices through generating neural networks using Keras.\n",
    "\n",
    "#### About the dataset \n",
    "The dataset used in this assessment has been downloaded from the UCI Machine Learning Repository https://archive.ics.uci.edu/ml/index.php. The dataset contains US census housing data related to various houses in areas around the city of Boston.This dataset contains 506 rows and 14 columns. \n",
    "\n",
    "The fourteen columns consist of the following data: \n",
    "\n",
    "|Column Name |     Column Description                                                      |\n",
    "|------------|-----------------------------------------------------------------------------|\n",
    "|CRIM        |Crime rates per town (CRIM)                                                  |\n",
    "|ZN          |Proportion of residential land zoned for lots over 25,000 sq.ft.             |\n",
    "|Indus       |Proportion of non-retail business acres per town                             |\n",
    "|Chas        |Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)        |\n",
    "|Nox         |Nitrogen oxides concentration (parts per 10 million)                         |\n",
    "|Rm          |Average number of rooms per dwelling                                         |\n",
    "|Age         |Proportion of owner-occupied units built prior to 1940                       |\n",
    "|Dis         |Weighted mean of distances to five Boston employment centres                 |\n",
    "|Rad         |Index of accessibility to radial highways                                    |\n",
    "|Tax         |Full-value property-tax rate per 10,000                                       |\n",
    "|Ptratio     | pupil-teacher ratio by town                                                  |\n",
    "|Black       | 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town               |\n",
    "|Lstat       | lower status of the population (percent)                                     |\n",
    "|Medv        | median value of owner-occupied homes in $1000s                               |\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data \n",
    "The Boston Housing dataset was imported into a jupyter notebook. The first five lines of the data were displayed as well as  the datatypes, to ensure that the data had been imported correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author Róisín Anglim 29-09-19\n",
    "# Import packages \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Add heading to csv file\n",
    "headings = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "#Read in data from a csv file\n",
    "df = pd.read_csv('housing.csv',delim_whitespace=True,names = headings) \n",
    "#Print first 5 lines of imported data \n",
    "df.head()\n",
    "#Set random seed for reproducability \n",
    "np.random.seed(42) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show data datatypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the Data\n",
    "\n",
    "The data was reviewed to identifying missing,duplicate or unnecessary data, in order to remove them from the analysis. \n",
    "\n",
    "Descriptive statistics was used on the data, to review if any columns contained a min value of zero. A minimum value of zero can indicate missing or invalid data.The only coulmns within this dataset that contained minimum values of zero were, ZN and CHAS. \n",
    "CHAS contains boolean values of 1 and 0, indicating whether the house is along the river or not.This column was eliminated from the review. ZN has been identified as having 372 cells with values equal to zero. ZN represents the amount of zoned land and will be zero for areas that have not been zoned. This coulmn was also eliminated.\n",
    "\n",
    "The pandas package was used to identify Null values and duplicate values. No cells within the dataset were identified as having either.\n",
    "\n",
    "Each variable within the dataset was plotted on a scatterplot inorder to identify outliers. No data was eliminated from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show descriptive statistics to identify missing or invalid data.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Identify the number of zero value cells\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify duplicate rows\n",
    "#df.duplicated(subset=None, keep=True)\n",
    "df[df.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Univariate Analysis\n",
    "#Plot scatterplot data to identify outliers \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig=plt.figure(figsize=(15, 15), dpi= 80, facecolor='w', edgecolor='k')    \n",
    "fig, ax = plt.subplots(figsize=(15,15), ncols=3, nrows=5)\n",
    "sns.swarmplot(y=\"DIS\",data=df,ax=ax[0][0])\n",
    "sns.swarmplot(y=\"ZN\",data=df, ax=ax[0][1])\n",
    "sns.swarmplot(y=\"NOX\",data=df,ax=ax[0][2])\n",
    "sns.swarmplot(y=\"RM\",data=df,ax=ax[1][0])\n",
    "sns.swarmplot(y=\"AGE\",data=df,ax=ax[1][1])\n",
    "sns.swarmplot(y=\"RAD\",data=df,ax=ax[1][2])\n",
    "sns.swarmplot(y=\"TAX\",data=df,ax=ax[2][0])\n",
    "sns.swarmplot(y=\"PTRATIO\",data=df,ax=ax[2][1])\n",
    "sns.swarmplot(y=\"B\",data=df,ax=ax[2][2])\n",
    "sns.swarmplot(y=\"LSTAT\",data=df,ax=ax[3][0])\n",
    "sns.swarmplot(y=\"MEDV\",data=df,ax=ax[3][1])\n",
    "sns.swarmplot(y=\"CRIM\",data=df,ax=ax[3][2])\n",
    "sns.swarmplot(y=\"INDUS\",data=df,ax=ax[4][0])\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferential Statistics \n",
    "#### Plot median house value distribution\n",
    "\n",
    "The data was split between houses alongside the river and houses not alongside the river. The median values of these properties were plotted.\n",
    "\n",
    "##### Observation\n",
    "There is a normal distribution for the houses along the river and a bimodal distribution between those not along the river."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = np.random.normal(size=100)\n",
    "## Select the data where the houses are along the river\n",
    "r= df.loc[df['CHAS'] == 1]\n",
    "## Select the data where the houses are not along the river\n",
    "s = df.loc[df['CHAS'] == 0]\n",
    "#Plot distributions of houses along the river and not along the river\n",
    "sns.distplot(r.MEDV)\n",
    "sns.distplot(s.MEDV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result - Independant T-Test\n",
    "An independant t-test was conducted on the two groups median house values. The first group r consisted of the houses along the river and the secound group s consisted of the houses not along the river. \n",
    "The results indicated that with a p-value 0f 0.07 the results are not significantly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Independant t-test\n",
    "\n",
    "ttest = stats.ttest_ind(r['MEDV'], s['MEDV'])\n",
    "print(ttest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix Review\n",
    "\n",
    "The correlation matrix was used to identify variables highly correlated to the median house prices.\n",
    "\n",
    "From the correlation matrix the percentage of the population that is lower class(LSTAT) and the number of rooms per house(RM), have the highest correlation 74% and 70%. As the number of rooms (RM) increase the median house prices (MEDV) increase. In contrast to this as the lower class popoulation percentage (LSTAT) increases the median house prices decrease (MEDV).\n",
    "\n",
    "The correlation matrix was used to check for multi-co-linearity. The variables RAD and TAX have a correlation of 91%. The variables DIS and Age have a correlation of 75%.NOX and INDUS have a correlation of 76%. Only one of each of the highly correlated variables will be selected for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###BI Variate analysis\n",
    "##Create correlation matrix\n",
    "\n",
    "plt.figure(figsize=(14,14))\n",
    "#Add coolwarn colour and annotations for each correlation\n",
    "sns.heatmap(df.corr(),vmin=-1,cmap='coolwarm',annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Distribution of Median House Prices\n",
    "\n",
    "The median house price distribution is plotted, to see that the house prices are normally distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots the distribution of the median house prices\n",
    "sns.distplot(df['MEDV'], bins=30)\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatterplot \n",
    "The scatterplot was used to identify variables with a linear relationship to the median house prices.\n",
    "It is evident from the plots that RM and LSTAT have a linear relationship. As the number of rooms (RM) increase the median value(MEDV) increases. Contrastingly as the percentage population of lower class increases, the median houses decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a multiple scatterplots for all variables against the target variable\n",
    "fig= plt.figure(figsize=(15, 15), dpi= 80, facecolor='w', edgecolor='k')\n",
    "fig, ax = plt.subplots(figsize=(15,15), ncols=3, nrows=4)\n",
    "sns.regplot(\"DIS\",\"MEDV\",df,ax=ax[0][0],scatter_kws={'alpha': 0.4})\n",
    "sns.regplot(\"ZN\",\"MEDV\",df, ax=ax[0][1],scatter_kws={'alpha': 0.4})\n",
    "sns.regplot(\"NOX\",\"MEDV\",df,ax=ax[0][2],scatter_kws={'alpha': 0.4})\n",
    "sns.regplot(\"RM\",\"MEDV\",df,ax=ax[1][0],scatter_kws={'alpha': 0.4})\n",
    "sns.regplot(\"AGE\",\"MEDV\",df,ax=ax[1][1],scatter_kws={'alpha': 0.4})\n",
    "sns.regplot(\"RAD\",\"MEDV\",df,ax=ax[1][2],scatter_kws={'alpha': 0.4})\n",
    "sns.regplot(\"TAX\",\"MEDV\",df,ax=ax[2][0],scatter_kws={'alpha': 0.4})\n",
    "sns.regplot(\"PTRATIO\",\"MEDV\",df,ax=ax[2][1],scatter_kws={'alpha': 0.4})\n",
    "sns.regplot(\"B\",\"MEDV\",df,ax=ax[2][2],scatter_kws={'alpha': 0.4})\n",
    "sns.regplot(\"LSTAT\",\"MEDV\",df,ax=ax[3][0],scatter_kws={'alpha': 0.4})\n",
    "sns.regplot(\"CRIM\",\"MEDV\",df,ax=ax[3][2],scatter_kws={'alpha': 0.4})\n",
    "sns.regplot(\"INDUS\",\"MEDV\",df,ax=ax[3][1],scatter_kws={'alpha': 0.4})\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Linear Regression Model\n",
    "\n",
    "#### Selecting Variables\n",
    "Based on the correlation matrix, highly correlated variables were removed from the model. The removed varaiable consisted of INDUS,RAD and AGE. These varaibles were highly correlated to NOX ,TAX and DIS and deemed redundant variables in the model generation.\n",
    "\n",
    "#### Split Test Train Data\n",
    "The data was seperated into test and training datasets. A model was then trained on 80% of the dataset and tested on 20% of the unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#This contains all columns excluding highly correlated variables =75%\n",
    "X = df.drop(columns =['MEDV','INDUS','RAD','AGE'])\n",
    "#This includes the target variable median house prices (MEDV)\n",
    "y = df[['MEDV']]\n",
    "\n",
    "# The train_test_split module splits the dataset into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control = Print test train split\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Model\n",
    "\n",
    "A linear regression model was generated to predict the median house prices based on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate & Print predictions\n",
    "lin_reg_mod = LinearRegression()\n",
    "lin_reg_mod.fit(X_train, y_train)\n",
    "pred = lin_reg_mod.predict(X_test)\n",
    "firstfivepredictions = np.array([pred[0:5]], dtype=object)\n",
    "print(firstfivepredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model\n",
    "\n",
    "The mean squared error was used to evaluate the model.It returned the average squared difference between the predicted values and the true values. The closer the value is to one, the better the model has performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show mean squared error\n",
    "test_set_rmse = (np.sqrt(mean_squared_error(y_test, pred)))\n",
    "print(test_set_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(abs(y_test- pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principle Component Analysis\n",
    "Based on the number high number of variables within the dataset a dimention reduction technique was used.\n",
    "Principle component analysis (PCA) is a dimention reduction technique. It works by taking highly correlated variables and transforming them into linearly uncorrelated principal components. PCA comonents have no correlation with eachother."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Scalability\n",
    "In order to prevent a large value from doiminating the results, the data was scaled.PCA is affected by scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns =['MEDV'])\n",
    "#This includes the target variable median house prices (MEDV)\n",
    "y = df[['MEDV']]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Ratio\n",
    "The PCA ratio was used to show the principle components which represent the most variance in the data.\n",
    "From the results you can see that PCA one and two combined account for 57% of the variance in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('Boston Housing Dataset Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatterplot PCA0 & PCA1\n",
    "\n",
    "A scatterplot was generated to investiagte if there was a strong correlation between the first and second PCA's, which account for 57% of the variance in the data. \n",
    "Based on the output below there is not a strong linear reltionship indicating that there is no relationship between the two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the function regplot to make a scatterplot\n",
    "sns.regplot(x=x_pca[0], y=x_pca[1])\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Keras to Predict House Prices\n",
    "\n",
    "The Keras package was used to create neural networks inorder to predict the median house prices. The data and required pakages were imported.\n",
    "\n",
    "All of the thirteen variables within the dataset were used to conduct the first prediction.\n",
    "\n",
    "The data was split between between test and training data 20% and 80% respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import parkages to create neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For building neural networks.\n",
    "import keras as kr\n",
    "\n",
    "# For interacting with data sets.\n",
    "import pandas as pd\n",
    "\n",
    "# For encoding categorical variables.\n",
    "import sklearn.preprocessing as preprocessing\n",
    "\n",
    "# For splitting into training and test datasets.\n",
    "import sklearn.model_selection as mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add heading to csv file\n",
    "headings = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "#Read in data from a csv file\n",
    "df = pd.read_csv('housing.csv',delim_whitespace=True,names = headings) \n",
    "#Print first 5 lines of imported data \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This contains all columns excluding the median house prices (MEDV)\n",
    "inputs = df.drop(columns =['MEDV'])\n",
    "#This only includes the column median house prices (MEDV)\n",
    "outputs = df[['MEDV']]\n",
    "inputs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into test & train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.41978194  0.28482986 -1.2879095  ... -1.45900038  0.44105193\n",
      "  -1.0755623 ]\n",
      " [-0.41733926 -0.48772236 -0.59338101 ... -0.30309415  0.44105193\n",
      "  -0.49243937]\n",
      " [-0.41734159 -0.48772236 -0.59338101 ... -0.30309415  0.39642699\n",
      "  -1.2087274 ]\n",
      " ...\n",
      " [-0.41344658 -0.48772236  0.11573841 ...  1.17646583  0.44105193\n",
      "  -0.98304761]\n",
      " [-0.40776407 -0.48772236  0.11573841 ...  1.17646583  0.4032249\n",
      "  -0.86530163]\n",
      " [-0.41500016 -0.48772236  0.11573841 ...  1.17646583  0.44105193\n",
      "  -0.66905833]]\n"
     ]
    }
   ],
   "source": [
    "#Scale input data to zero mean and unit variance\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaled_inputs = scaler.fit_transform(inputs)\n",
    "print(scaled_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "inputs_train, inputs_test, outputs_train, outputs_test = mod.train_test_split(scaled_inputs, outputs, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    model =  kr.models.Sequential()\n",
    "    model.add(kr.layers.Dense(units=13, activation='relu', input_dim=13))\n",
    "    model.add(kr.layers.Dense(units=6, activation='relu'))\n",
    "    model.add(kr.layers.Dense(units=1, activation='relu'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -365.139851 using {'batch_size': 20, 'epochs': 10}\n"
     ]
    }
   ],
   "source": [
    "#https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n",
    "# define the grid search parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "estimator = KerasClassifier(build_fn=model, verbose=0)\n",
    "batch_size = [10, 20] #40, 60, 80, 100\n",
    "epochs = [5, 10] #50, 100\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "np.random.seed(42) \n",
    "grid = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=3,scoring='neg_mean_squared_error')\n",
    "grid_result = grid.fit(inputs_train, outputs_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=3, error_score='raise',\n",
      "       estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x000001E5AADC7D48>,\n",
      "       fit_params=None, iid=True, n_jobs=1,\n",
      "       param_grid={'batch_size': [10, 20], 'epochs': [5, 10]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
      "       scoring='neg_mean_squared_error', verbose=0)\n"
     ]
    }
   ],
   "source": [
    "print(grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_103 (Dense)            (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 6)                 84        \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 273\n",
      "Trainable params: 273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42) \n",
    "estimator.fit(inputs_train,outputs_train,epochs=10, batch_size=10)\n",
    "estimator.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 64.94666 ]\n",
      " [108.33042 ]\n",
      " [ 31.055744]\n",
      " [ 74.72299 ]\n",
      " [ 30.492126]\n",
      " [ 31.727757]\n",
      " [ 22.374405]\n",
      " [ 26.869854]\n",
      " [ 22.901646]\n",
      " [ 17.271526]\n",
      " [ 52.945625]\n",
      " [ 58.99697 ]\n",
      " [ 25.439621]\n",
      " [ 39.022408]\n",
      " [ 35.38611 ]\n",
      " [ 25.7136  ]\n",
      " [ 24.325466]\n",
      " [ 25.450935]\n",
      " [156.42442 ]\n",
      " [ 30.644394]\n",
      " [ 85.15689 ]\n",
      " [ 89.56438 ]\n",
      " [ 12.379537]\n",
      " [ 32.128784]\n",
      " [ 29.450054]\n",
      " [ 21.566307]\n",
      " [ 46.457924]\n",
      " [ 26.718246]\n",
      " [ 49.272232]\n",
      " [ 16.196363]\n",
      " [ 42.87998 ]\n",
      " [ 57.80353 ]\n",
      " [ 33.72551 ]\n",
      " [ 29.688103]\n",
      " [ 24.21989 ]\n",
      " [ 24.699509]\n",
      " [ 92.394325]\n",
      " [ 53.052505]\n",
      " [ 55.836678]\n",
      " [ 71.34075 ]\n",
      " [  9.902167]\n",
      " [ 76.096596]\n",
      " [153.7228  ]\n",
      " [ 64.783066]\n",
      " [ 70.19864 ]\n",
      " [ 28.853905]\n",
      " [ 12.759884]\n",
      " [ 74.77752 ]\n",
      " [ 23.200123]\n",
      " [101.557945]\n",
      " [ 66.7333  ]\n",
      " [122.71309 ]\n",
      " [ 16.976093]\n",
      " [ 36.08438 ]\n",
      " [119.81691 ]\n",
      " [ 17.382313]\n",
      " [ 33.137627]\n",
      " [107.22236 ]\n",
      " [ 74.03714 ]\n",
      " [ 45.75219 ]\n",
      " [ 77.93365 ]\n",
      " [142.2065  ]\n",
      " [ 78.97439 ]\n",
      " [ 20.067814]\n",
      " [120.03788 ]\n",
      " [ 36.380157]\n",
      " [ 32.265038]\n",
      " [ 85.17363 ]\n",
      " [115.13084 ]\n",
      " [ 25.277016]\n",
      " [ 66.753395]\n",
      " [ 21.73444 ]\n",
      " [ 27.255688]\n",
      " [101.52427 ]\n",
      " [ 41.084423]\n",
      " [ 24.435593]\n",
      " [ 19.884254]\n",
      " [150.36015 ]\n",
      " [ 30.842323]\n",
      " [ 27.886234]\n",
      " [ 76.403694]\n",
      " [ 24.555237]\n",
      " [131.86897 ]\n",
      " [ 25.137005]\n",
      " [ 32.45497 ]\n",
      " [ 75.962616]\n",
      " [ 29.657661]\n",
      " [ 63.147034]\n",
      " [ 63.870064]\n",
      " [ 28.643105]\n",
      " [ 69.49015 ]\n",
      " [ 24.005928]\n",
      " [ 27.088173]\n",
      " [ 67.054214]\n",
      " [ 18.964825]\n",
      " [ 27.188148]\n",
      " [ 29.706131]\n",
      " [ 24.635689]\n",
      " [ 26.684582]\n",
      " [ 24.997047]\n",
      " [ 64.168205]\n",
      " [ 46.569946]]\n"
     ]
    }
   ],
   "source": [
    "# Have the network predict the classes of the test inputs.\n",
    "predictions = estimator.model.predict(inputs_test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1814.5990384850602"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(outputs_test,predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate error in model (Using mean absolute error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MEDV    31.511379\n",
       "dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\n",
    "\n",
    "np.mean(abs(predictions - outputs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### References \n",
    "\n",
    "[2] Data Cleaning with Python and Pandas: Detecting Missing Values https://towardsdatascience.com/data-cleaning-with-python-and-pandas-detecting-missing-values-3e9c6ebcf78b (Accessed 21.11.19)\n",
    "\n",
    "[3]Dimension Reduction Techniques with Python https://towardsdatascience.com/dimension-reduction-techniques-with-python-f36ca7009e5c (Accessed 21.11.19)\n",
    "\n",
    "[4]SciKit Learn https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA (Accessed 21.11.19)\n",
    "\n",
    "[5] A complete guide to scatterplots https://chartio.com/learn/charts/what-is-a-scatter-plot/ (Accessed 24.11.19)\n",
    "\n",
    "[6] Linear Regression on Boston Housing Dataset https://towardsdatascience.com/linear-regression-on-boston-housing-dataset-f409b7e4a155 (Accessed 03.12.19)\n",
    "\n",
    "[7] Example of Multiple Linear Regression in Python https://datatofish.com/multiple-linear-regression-python/ (Accessed 04.12.19)\n",
    "\n",
    "[8] Implementing PCA in Python with Scikit-Learn https://stackabuse.com/implementing-pca-in-python-with-scikit-learn/ (Accessed 4.12.19)\n",
    "\n",
    "[9] Building our first neural network in keras https://towardsdatascience.com/building-our-first-neural-network-in-keras-bdc8abbc17f5 Accessed (06.12.19)\n",
    "\n",
    "[10] Multicollinearity https://en.wikipedia.org/wiki/Multicollinearity (06.12.19)\n",
    "\n",
    "[11] GeekForGeeks Python | Mean Squared Error https://www.geeksforgeeks.org/python-mean-squared-error/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
